{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformer (ViT) for Image Classification [5 points]\n",
        "Use a Vision Transformer to solve the Cats and Dogs Dataset. You can use pre-defined ViT model or implement from scratch.\n",
        "Deploy the model and record a short video (~5 mins) on how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vyW0stza6aSj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import timm\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch import nn, optim\n",
        "import PIL\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4G_ebmcAcuL",
        "outputId": "6232a279-88ff-4ee3-ede3-484c9fae2b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ],
      "source": [
        "zip_file_path = \"/content/sample_data/kagglecatsanddogs_5340.zip\"\n",
        "extract_path = \"kagglecatsanddogs_5340\"\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted successfully!\")\n",
        "dataset_path = \"kagglecatsanddogs_5340/PetImages\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "#dataset = ImageFolder(root=dataset_path, transform=transform)\n",
        "class CustomImageFolder(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root, transform)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            return super().__getitem__(index)\n",
        "        except (PIL.UnidentifiedImageError, OSError, ValueError) as e:\n",
        "            print(f\"Skipping corrupted image: {self.imgs[index][0]}\")\n",
        "            return self.__getitem__((index + 1) % len(self.imgs))\n",
        "\n",
        "dataset = CustomImageFolder(root=dataset_path, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "294nST3yAoWC"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load ViT model\n",
        "model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, num_classes=2)\n",
        "model = model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DCv4BNqAtjy",
        "outputId": "c4f2466f-07f1-45b5-b97f-c0d3f40e70ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/PIL/TiffImagePlugin.py:949: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Dog/11702.jpg\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Cat/666.jpg\n",
            "Epoch 1, Loss: 0.0827, Accuracy: 96.88%\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Dog/11702.jpg\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Cat/666.jpg\n",
            "Epoch 2, Loss: 0.0588, Accuracy: 97.78%\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Dog/11702.jpg\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Cat/666.jpg\n",
            "Epoch 3, Loss: 0.0521, Accuracy: 98.08%\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Cat/666.jpg\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Dog/11702.jpg\n",
            "Epoch 4, Loss: 0.0474, Accuracy: 98.19%\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Dog/11702.jpg\n",
            "Skipping corrupted image: kagglecatsanddogs_5340/PetImages/Cat/666.jpg\n",
            "Epoch 5, Loss: 0.0386, Accuracy: 98.55%\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct/total:.2f}%\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "train_model(model, train_loader, val_loader, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Deploy your trained ViT model. This could be a simple script or application that takes an image as input and predicts whether it's a cat or a dog.\n",
        "\n",
        "The model is deployed here - https://huggingface.co/spaces/ruthvikvkumar/cat_dog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Yki5WhCA2k4",
        "outputId": "3c1c5d1e-aeb6-4297-fd6d-bd5a3f7d3f8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction - 1: Cat\n",
            "Prediction - 2: Dog\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"vit_cats_dogs.pth\")\n",
        "\n",
        "def predict(image_path, model):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = transform(image).unsqueeze(0).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        _, predicted = torch.max(output, 1)\n",
        "    return \"Dog\" if predicted.item() == 1 else \"Cat\"\n",
        "\n",
        "# Test prediction\n",
        "sample_image = \"/content/sample_data/cat.jpg\"\n",
        "print(\"Prediction - 1:\", predict(sample_image, model))\n",
        "\n",
        "sample_image1 = \"/content/sample_data/dog.jpg\"\n",
        "print(\"Prediction - 2:\", predict(sample_image1, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Record a short video (~5 mins) demonstrating how your deployed ViT model works. The video should showcase the model taking image inputs and providing predictions. Explain the key aspects of your implementation and deployment process in the video.\n",
        "   a. Upload the video to UBbox and create a shared link\n",
        "   b. Add the link at the end of your ipynb file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Shared UBbox Video Link:**\n",
        "https://buffalo.box.com/s/3argmh5tl2skykkita4skkve0bju5cul"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References:\n",
        "1. https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
        "2. https://huggingface.co/timm/vit_base_patch16_224.augreg2_in21k_ft_in1k"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
